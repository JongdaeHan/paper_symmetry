%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[symmetry,article,submit,moreauthors,pdftex]{Definitions/mdpi} 

% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerospace, agriculture, agriengineering, agronomy, ai, algorithms, allergies, analytica, animals, antibiotics, antibodies, antioxidants, appliedchem, applmech, applmicrobiol, applnano, applsci, arts, asi, atmosphere, atoms, audiolres, automation, axioms, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomechanics, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, brainsci, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coatings, colloids, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, crops, cryptography, crystals, curroncol, cyber, dairy, data, dentistry, dermato, dermatopathology, designs, diabetology, diagnostics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, entropy, environments, environsciproc, epidemiologia, epigenomes, fermentation, fibers, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fractalfract, fuels, futureinternet, futuretransp, futurepharmacol, futurephys, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, hazardousmatters, healthcare, hearts, hemato, heritage, highthroughput, histories, horticulturae, humanities, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijtm, ijtpp, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jcdd, jcm, jcp, jcs, jdb, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jzbg, kidney, land, languages, laws, life, liquids, literature, livers, logistics, lubricants, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, metabolites, metals, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, minerals, mining, modelling, molbank, molecules, mps, mti, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nri, nursrep, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photochem, photonics, physchem, physics, physiolsci, plants, plasma, pollutants, polymers, polysaccharides, proceedings, processes, prosthesis, proteomes, psych, psychiatryint, publications, quantumrep, quaternary, qubs, radiation, reactions, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, risks, robotics, safety, sci, scipharm, sensors, separations, sexes, signals, sinusitis, smartcities, sna, societies, socsci, soilsystems, solids, sports, standards, stats, stresses, surfaces, surgeries, suschem, sustainability, symmetry, systems, taxonomy, technologies, telecom, textiles, thermo, tourismhosp, toxics, toxins, transplantology, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, vetsci, vibration, viruses, vision, water, wevj, women, world 

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
% MDPI internal commands
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2021}
\copyrightyear{2020}
%\externaleditor{Academic Editor: Firstname Lastname} % For journal Automation, please change Academic Editor to "Communicated by"
\datereceived{} 
\dateaccepted{} 
\datepublished{} 
\hreflink{https://doi.org/} % If needed use \linebreak
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, paracol, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Effects of Adversarial Training on the Safety of Classification Models}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Effects of Adversarial Training on the Safety of Classification Models}

% Author Orchid ID: enter ID or remove command
% \newcommand{\orcidauthorA}{0000-0000-0000-000X} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Handong Kim $^{1}$ and Jongdae Han *}%\orcidA{} and Jongdae Han *}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Handong Kim and Jongdae Han}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Handong Kim., Jongdae Han.}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{$^{1}$ \quad Affiliation 1; aggsae@gmail.com}

% Contact information of the corresponding author
\corres{Correspondence: elvenwhite@smu.ac.kr}

% Current address and/or shared authorship
% \firstnote{Current address: Affiliation 3} 
% \secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\)
\abstract{
Artificial Intelligence(AI) is one of the important topic that implements symmetry in computer science. As like humans, most AI also learns by trial-and-error approach which requires appropriate adversarial examples. In this study, we prove that adversarial training can be useful to verify the safety of classification model in early stage of development. 
We experimented with various amount of adversarial data and found that the safety can be significantly improved by appropriate ratio of adversarial training.
}

% Keywords
\keyword{Requirements Engineering; Nonfunctional Requirements; Safety; Artificial Intelligence; Adversarial Training;} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
% \encyclopediadef{Instead of the abstract}
% \entrylink{The Link to this entry published on the encyclopedia platform.}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\section{Introduction}


Artificial intelligence(AI) is one of the important topic that implements symmetry in computer science. As humans learn how to recognize objects by trial-and-error approach, AI that utilizes supervised learning technique also learns using similar approach. But sometimes it is overlooked that we need to supply not only correct data but erronous data to make use of such symmetry.

Software requirements are one of the important factors for software quality.
Verifying the requirements in an early stage of software development life cycle induces higher software quality and lower cost.
Requirements can be divided into functional requirements and nonfunctional requirements(NFR).
Functional requirements are fullfillments of functional user expectations, while other requirements are considered NFRs. Typical NFRs include performance(whether software performs the desired behavior, ex. accuracy), fairness \cite{fairness-microsoft, fairness-dwork, fairness-feldman, fairness-tramer, fairness-zhang, fairness-zemel}, securit y\cite{security-mei, security-mei2, security-barreno}, safety(whether the execution result of the software is safe)  \cite{safety-amodei,safety-juric,safety-leike}, and transparency(whether software can be trusted) \cite{transparency-yosinski,xai-transparency-ribeiro,xai-transparency-murdoch}.
It is essential to verifty not only functional requirements but also NFRs for high-quality software.

Traditionally, there are well-known methods to verify requirements including throughful documentation of requirement specification, model-based verification, etc. These methods focus on making detailed documentation and testing at each stage of the software development lifecycle. Hereafter, developers can directly check the operation of code; thus, functional requirements and NFRs can be verified through codes.

Nevertheless, with the advent of deep neural network which has different characteristics from existing common software, it is difficult to verify requirements through traditional methods. 
Performance, which is one of the most widely applied NFRs, can be verified to some extent with accuracy or F1 score. Most data-driven deep neural networks have some kind of accuracy as a critical criteria. However, other NFRs such as safety are difficult to verify because of deep neural network's 'black-box' property.
It is difficult to apply traditional verification methods for deep neural networks because they operates as a black box model. In traditional softwares, NFRs can be verified through codes, but it is difficult to verify deep neural network with code because the model is not a explainable representation of logical flow.
Meanwhile, the verification of NFRs is very important for deep neural networks especially if they are applied for mission- or safety-critical system, such as autonomous driving \cite{ai-driving-bajarski,ai-driving-levinson}, medical diagnosis \cite{ai-medical-ramsundar,ai-medical-vieira,ai-medical-xai-holzinger,ai-medical-krause,ai-medical-tan,ai-medical-pesapane,ai-medical-miller}, and finance \cite{ai-finance-fu}.
If NFRs such as safety, security, and transparency are not guaranteed, the application of newer AI techniques will be risky due to liability issues. 
While currently quality assuarance of AI is performed only on the performance corresponding to how well AI through methods such as accuracy and F1 score, there should be more considerations upon other NFRs to apply those models to variety of industrial domains.

Several studies have been conducted in this regard, and those studies can be categorized into two.
The first category directly deals with various types of AI-inducing NFRs.


\begin{itemize}
    \item Studies have defined the fairness of AI, addressed problems that may arise when fairness is not verified, and proposed methods for verifying fairness \cite{fairness-dwork, fairness-feldman, fairness-tramer, fairness-zhang, fairness-zemel}.
    \item In \cite{security-barreno,security-mei,security-mei2}, security problems that can arise due to the vulnerability of AI were discussed. Additionally, attack methods threatening the security of AI are explained and methods to prevent them were proposed.
    \item In \cite{transparency-yosinski}, the authors proposed a method for understanding the behavior of AI by visualizing the activation of each layer of neural network models.
    \item Some researchers \cite{safety-amodei,safety-juric,safety-leike} defined safety of AI, proposed safety methods, and studied safety-violation cases.
\end{itemize}

The other category is consisted with studies investigating problmes related to the NFRs of AI, although they do not directly address them.
Explainable AI(XAI) and adversarial training are one of topics of such studies.
XAI are expected to enable people to understand the process of training and deriving the results of AI. 
XAI is expected to validate AI model by providing more transparency \cite{xai-arrieta,xai-samek,xai-transparency-murdoch,xai-transparency-ribeiro,ai-medical-xai-holzinger}.
In the other hand, concept of advesarial training is introduced, which is making trained models more robust against adversarial examples \cite{adversarial-goodfellow,adversarial-kurakin,adversarial-kurakin2,adversarial-papernot,adversarial-szegedy,adversarial-yuan}.
Although the main purpose of these studies is to maintain or improve the performance of models against adversarial examples, it can be interpreted as a verification method to properly classify adversarial examples that threaten the safety of AI models.
As those studies explain the learning process of models with adversarial examples, they do not address applying adversarial training for verification of AI NFRs.
Therefore, in our study, on the basis of the above-cited studies, we will verify NFRs of AI, especially the \textit{safety} of classification models using \textit{adversarial training}.
The details of our research is discussed in section 3.

In this study, we will show the safety of classification models can be verified using adversarial examples as a part of data preparation for training(adversarial training).
To prove this, we set up following research questions(RQs).

\begin{description}
    \item[RQ1.]	Can classification models appropriately classify adversarial examples by adversarial training?
    \item[RQ2.]	Do they show same results as RQ1 for safety-related datasets?
    \item[RQ3.] Does adversarial training affect safety regardless of the model used for training?
    \item[RQ4.]	At what rate adversarial examples should be included with normal data to archive balance between safety and accuracy?
\end{description}

With experiments, we could find out following:

\begin{enumerate}
    \item	Adversarial training can improve safety. Depending on the experimental results, safety which is defined as successful classification of adversarial examples is improved by 20\%--40\%.
    \item	Adversarial training can be used to verify safety of AI. Experimental results show that adversarial training can affect safety improvement, means it can be used as verification method to ensure safety by applying certain amount of adversarial examples to training dataset.
    \item   Adversarial training can be utilized as a factor to verify safety regardless of the model structure. Improvement in safety is observed regardless Of the model structure.
    \item	We propose a process of adjusting the adversarial-to-normal ratio in training datasets according to the preferential requirements. Experimental results with different adversarial example inclusion ratios show that adversarial inclusion ratios play key roles in a tradeoff between accuracy and safety.
\end{enumerate} 

The rest of this paper is structured as follows. In section 2, we review previous studies on verification methods for AI NFRs.
In section 3, we define the safety of software and show the experiment design applied in this study.
In section 4, we explain experiments conducted according to the designed experimental methods, and analyze the results in section 5.
We conclude the paper and recommend future work in section 6.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}

Prior studies have directly and indirectly addressed the NFRs verification of AI.

\subsection{Studies that directly addressing the NFRs of AI}

\subsubsection{Fairness of AI}

With current supervised learning techniques, AI learns the features of data and derives the results of input data based on the learned features.
Because of these characteristics, if there is a bias in the training dataset, discriminatory results should be derived by AI.
As explained in \cite{fairness-dwork, fairness-zemel}, bias in training datasets can lead to discriminatory results according to features such as race, gender, and age of each individual.
To address this matter, the authors proposed a statistical method to find out  discriminating elements of training datasets, argued that it is important to keep the fairness of AI as data bias is directly related to privacy.
Feldman et al. \cite{fairness-feldman} addressed the problem of unfairness that caused by discriminatory features of datasets  and a method to solve such problem.
Zhang et al. \cite{fairness-zhang} showed that such imbalance is orginated from not only data but also data features, and lead in worse performance.
To improve the fairness of AI, \cite{fairness-feldman,fairness-microsoft, fairness-tramer} ensured the fairness of AI by removing or finding factors, such as bias in datasets.

\subsubsection{Security of AI}

AI is recently applied to areas even closely related to human privacy.
Due to such trend, the more it demands security for sensitive, private information.
Studies have been conducted to identify and classify various security problems occurring because of vulnerabilities in AI and attacks that can cause security problems \cite{security-barreno}.
There are studies that suggest attack methods that can threaten security through these vulnerabilites and defense methods to defend them.
Mei et al. \cite{security-mei2} proposed the attack method that can threaten the security against tools widely used in actual data analysis.
Another study by same authors \cite{security-mei} proposed the method to attack security through injecting malicious data into training data and technique to prevent such attacks.

\subsubsection{Transparency of AI}

Most of the AI currently applied to operate in black-box manner, therefore users cannot explain how can certain results were derived from models.
Lack of transparency in AI due to black-box operations can lead to complex issues, including legal issues regarding who is responsible when problems arise due to AI's decisions.
A study \cite{transparency-yosinski} proposed two tools to visualize layers activated in the process of deriving results for convolutional network-based deep learning models.
By analyzing visualized layers, users can make sure whether the corresponding result is appropriately derived and prevent problems in advance.

\subsubsection{Safety of AI}

Safety is usually defined as a state in which or a place where you are safe and not in danger or at risk.
To apply the definition with AI, the decision of AI should not pose any danger or risks to humans while being used in various domains.
Safety is especially important in domains such as autonomous driving, where AI decisions have a significant impact on humans.
If safety is not properly verified throughout software development using AI, it can lead to serious results.
Amodei addressed safety issues that occur in existing AI studies \cite{safety-amodei}. 
In the study, examples showed safety can be often violated by prioritizing performance.
Another studies \cite{safety-juric,safety-leike} described the concept and definition of safety in AI and proposed general methods for achieving safety.

\subsection{Studies that indirectly addressing NFRs of AI}

In the previous section, we discussed studies that directly addressing NFRs of AI.
There are other studies addressing NFRs of AI.
For example, XAI and adversarial training are closely related to transparency and safety.
In this section, we disscuss studies regarding XAI and adversarial training and how they indirectly address NFRs of AI.

\subsubsection{Explainable AI(XAI)}

Although main purpose of XAI is not to verify AI NFRs, there are some studies explaining transparent learning and result derivation process in AI can affect some of NFRs.
Due to black-box characteristics of emerging deep neural networks, derived results from such model cannot be trusted without proper explanation.
Particularly, it is more important to solve these problems in fields where results have a large impact on humans, such as autonomous driving and medical diagnosis.
To solve this problem, XAI applied to enable humans to look into the AI learning process.
There are well known studies which defined the concept of XAI and introduced the methodology used  \cite{xai-samek, xai-arrieta,ai-medical-xai-holzinger,xai-transparency-ribeiro,xai-transparency-murdoch} .

\subsubsection{Adversarial examples and attacks}

The main purpose of adversarial examples and attacks are not verifying AI NFRs.
However, in \cite{adversarial-kurakin}, it was noted that

\begin{quote}
   "a good response to adversarial examples is an important safety issue in AI."
\end{quote}

Therefore, adversarial examples can be crucial part of safety of AI.

Many studies have shown that the characteristics of AI learning algorithms are vulnerable to adversarial examples, which behave as adversarial examples regardless of the structure and training data of the models\cite{adversarial-szegedy,adversarial-goodfellow}.
Goodfellow et al. \cite{adversarial-goodfellow} proposed a fast gradient sign method(FGSM) that uses gradients from the learning process to generate adversarial examples. We chose FGSM as our primary adversarial attack method in our experiment.
Another study \cite{adversarial-kurakin2} demonstrated that data from real world can be act as adversarial examples against models trained within "laboratory" environment. In their experiment, they made some pictures with a smartphone camera and applied them with established dataset, resulting in erronous results.
Papernot et al. \cite{adversarial-papernot} proposed a method for adversarial attacks in a black box environment, with no information regarding the network the training dataset, without knowing structure of the neural network and properties of training dataset. The study shows possibility of adversarial attacks in production environment.
Authors in \cite{adversarial-yuan} summarized and classified various methods for generating adversarial examples. Although we have not used all of the method in our experiment, our experiment shows a general tendency rather than method-by-method comparison.
Several studies \cite{adversarial-poon,adversarial-terzi,adversarial-dong} proposed methods to utilize adversarial training while training classification models. While these methods definitely improves training process, they re-defines performance metric regarding adversarial example, whereas our proposed process let human superviser set a balance between multiple requirement including performance.

These studies insist that adversarial attacks can be threats to certain NFRs. Therefore, we conducted experiments to show NFR verification can be done by appropriate use of adversarial examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview of experiments}

In this section, we define safety and present an overview of experiments to prove our RQs.

\subsection{Definition of safety}

AI must ensure minimum safety regardless of its purpose. In a field where AI is applied, the actual input data may not exist in the dataset used for training.
Additionally, AI suffers from multiple threats in real-world applications, such as adversarial attacks(adversarial examples).

\begin{figure}[H]
\includegraphics[width=13 cm]{Definitions/example-of-adversarial.png}
\caption{Examples of adversarial examples.\label{example-of-adversarial}}
\end{figure}   

Examples of adversarial attacks are shown in Figure~\ref{example-of-adversarial}. Both signs looks same in human eyes. Nevertheless, the right sign is slightly skewed with an adversarial attack is classified as different sign with left one. 
If only left sign is used in training autonomous driving model and right sign is given as real world input, it can cause fatal safety problems. Therefore, it is important to consider adversarial attacks while training model to ensure safety.
Therefore we define safety as following:

\begin{quote}
    "a measure of whether a model responds appropriately to data with untrained features or to data that have been adversarially attacked to obtain incorrect results from the model."
\end{quote}

Practically, it is difficult to collect data exhastively with consideration of every situations. And even if they are able to be collected, a lot of resources are required to process and train them.
Then which data should be learned first to ensure required safety?

\subsection{Overview of our experiments}

We assume that adversarial examples are given the highest priority and are the data that should be affect model behaviour.
This is because adversarial examples are data designed to cause problems or confuse the results of the model.
 Figure~\ref{example-of-adversarial} shows adversarial example of traffic signs for autonomous driving. As shown in the figure, traffic signs with certain natural condition(sunlight, fog, etc.) might lead in erronous decision for the driving system. 
Creating and configuring datasets can also cause another problem. Generating corresponding data for every possible conditions is difficult because there are too many possibility in the environment in which the model is being applied.
For these reasons, adversarial training can be a way to ensure AI safety, which has been proven through experiments.
If adversarial training allows a model to better respond to adversarial examples to improves safety performance, it can be used to verify whether safety is guaranteed in advance by preparing adversarial examples before training and mixing them with the training data.

To prove this, we first prepared different kinds of dataset and used them to generate adversarial datasets with different strength of artificial transform. We define it as degree of transformation \begin{math}\epsilon\end{math}.
Several degrees of \begin{math}\epsilon\end{math} were prepared to find meaningful \begin{math}\epsilon\end{math} for generating adversarial examples.
If \begin{math}\epsilon\end{math} is too small, transformed data will have little difference from original data. Therefore, it cannot deliver meaningful change.
If \begin{math}\epsilon\end{math} is too large, there are too much perturbations in adversarial examples that human can find the differences. Thus, it can not be used as meaningful adversarial examples either.
To maintain consistency, the size of training dataset is fixed regardless \begin{math}\epsilon\end{math} to control effect of size of training dataset over performance.

Furthermore, to find out at what ratio adversarial examples should be included for safety verification while preparing data, we prepared multiple datasets by setting different mixing ratio of the original data and the adversarial examples.
Because former study shows adversarial examples tend to decrease \cite{trade-off}, we intend to find balance between safety and accuracy.
In order to maintain the accuracy of the model learned with the original dataset with limitedresources, we populated the original data with adversarial examples at an appropriate ratio while preserving the size of the training dataset.
Accuracy represents how well a model classifies the {\it original} validation data, while safety represents how well model classifies the {\it adversarial} validation data, which is considered threatening model according to the definition given above.
With the belief that results may differ according to the characteristics of the model's structure, we prepared learning models with different structures. 
Detailed informations regarding the dataset, adversarial attack method, accuracy, safety, models, and experimental methods are discussed in section 4.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

We conducted experiments under different conditions to answer formulated RQs. To control effects from model structure and data characteristics, we prepared three datasets and three different models.
Section 4.1 introduces datasets and models used in our experiment and the reason they were selected.
Section 4.2 presents a brief introduction to FGSM algorithm used for generating adversarial examples and the reason it is was selected.
Section 4.3 describes detailed experimental processes designed in section 3.

\subsection{Datasets and Models}

\subsubsection{Datasets}

For the experiments, three datasets including CIFAR-10, CIFAR-100\cite{cifar}, and German traffic sign recognition benchmark(GTSRB)\cite{gtsrb}, were used.
CIFAR-10 comprises 60,000 data in 10 classes, each containing 6,000 pictures with three channels and \begin{math}{32\times 32}\end{math} in size.
We used 5,000 data per each class for training and the remaining were used for evaluation.
Figure~\ref{cifar-datasets} shows examples from CIFAR datasets.

\begin{figure}[H] 
\includegraphics[width=13cm]{Definitions/cifar-datasets.png}
\caption{Examples from CIFAR datasets.\label{cifar-datasets}}
\end{figure} 


CIFAR-100 comprises 60,000 data in 100 classes, each class containing 600 pictures with three channels and \begin{math}{32\times 32}\end{math} in size. We used 500 data per each class for training, and the remaining were used for evaluation.
CIFAR-10 and CIFAR-100 were used for the experiment to answer RQ1, which corresponds to the most basic hypothesis established herein.
There is difference between CIFAR-10 and CIFAR-100. CIFAR-10 has a large number of data for small number of classes, means it is likely for the model to successfully learn features for each class.
Conversely, CIFAR-100 has a small number of data for many classes, so the model is less likely to learn features for each class.
If it is possible to identify the improvement of safety through adversarial training for datasets with these opposite characteristics, we can reliably expect that improvement of safety will occur regardless data characteristic. Therefore, we used both CIFAR datasets for the experiment.

\begin{figure}[H]
\includegraphics[width=13 cm]{Definitions/gtsrb-dataset.png}
\caption{Examples from GTSRB dataset\label{gtsrb-dataset}}
\end{figure} 

German Traffic Sign Recognition Benchmark (GTSRB) is a German traffic sign dataset. It comprises more than 50,000 data under 43 classes.
Figure~\ref{gtsrb-dataset} shows some examples from GTSRB data.
One of the difference between GTSRB and CIFAR datasets is consistency between the number of data corresponding to each class.
By adding adversarial examples, numbers of the data per class can be not uniform.
Therefore, we selected GTSRB because we assume if it is possible to identify the safety improvement of a model through adversarial training for datasets in which the number of data is not uniform for each class, we can get valid results for other datasets.
Another important reason for selecting GTSRB is that it is closely related to safety.

\subsubsection{Models}

Three models with different structures were applied to understand the effect of adversarial training through data augmentation on safety performance regardless of the network structure.
LeNet\cite{lenet} is a classic convolutional neural network-structured model developed in 1998 with simple structure.
LeNet has the simplest structure among the models used in the experiment here. It was used to understand the effect of simple model.
ResNet18\cite{resnet} and VGG16\cite{vgg} are the most well-known and widely used models for classification, and their performance has also been proven through ImageNet large scale visual recognition challenge.
Although VGG16 came second in the 2014 competition, it is more popular than the winning model GoogLeNet because of its simplicity and ease of use.
ResNet18 won the 2015 competition with the addition of Residual blocks to VGGNet.
We selected two models with well-known classification performance beween researchers.
For LeNet, we directly implemented the model by referring to the paper and used it.
For ResNet18 and VGG16, models defined in {\it "torchvision.models"} of PyTorch\cite{pytorch}, one of the deep learning frameworks used for Python were used.
Fine-tuning for the models were intentionally omitted to show side-effect of adversarial examples over accuracy. If we fine-tunes these models more, there is possibility models can reach some "golden" status regardless how much we put into adversarial examples, effectively reducing its side-effect. Because it is hard to measure and control resources used to fine-tuning, we decided to use models with its default condition.

\subsection{FGSM}

We define that the safety of AI is how well it classifies adversarial examples mentioned in section 3.
To create adversarial examples for experiment, we used FGSM \cite{adversarial-goodfellow} to generate adversarial examples. Because FGSM generates adversarial examples using gradient in the training process, it is necessary to train at least once with the dataset that we want to generate adversarial examples.
Therefore, we used LeNet which has a relatively simple structure and does not require much time to learn in our paper.

\begin{figure}[H] 
\includegraphics[width=13cm]{Definitions/fgsm.png}
\caption{Process of generating adversarial examples with FGSM. Image adapted from \cite{adversarial-goodfellow}\label{fgsm}}
\end{figure} 

As shown in Figure~\ref{fgsm}, adversarial examples were generated using the gradient change that occuring during the training process.
At this time, \begin{math}\epsilon\end{math} was used to determine the degree of adversarial attack by reflecting the slope value.
We tried \begin{math}\epsilon\end{math} of 0.05 and 0.1, due to \begin{math}\epsilon\end{math} above 0.1 makes the transformation too obvious to even human observation. Such examples are not able to serve as an adversarial example. Therefore, two types of \begin{math}\epsilon\end{math} are used to generate adversarial examples.
In \cite{adversarial-goodfellow}, adversarial examples were applied only to the validation data to confirm the effect of adversarial examples. But because our goal is to confirm that adversarial examples are also useful to verify safety in AI, we applied adversarial example to training data also. 
\begin{figure}[H]
\includegraphics[width=13 cm]{Definitions/adversarial-dataset.png}
\caption{Examples of generated adversarial examples\label{adversarial-examples}}
\end{figure} 

As shown in Figure~\ref{adversarial-examples}, adversarial examples were generated for CIFAR-10, CIFAR-100, and GTSRB data to be used for experiments.
In this paper, the training and evaluation data to which the adversarial attack is applied are referred as {\it adversarial training data} and {\it adversarial evaluation data}.
Generated adversarial training data were randomly extracted and used for populating original training data at a certain ratio and are used for training, while the adversarial evaluation data were used for experiments to evaluate the safety.

\subsection{Methodology}

We conducted the experiments using a PC with Ubuntu 18.04 OS, 4.20GHz Intel(R) i7-7700K CPU, 64GB RAM, and NVIDIA Titan XP 12GB GPU.

The models used in the experiment have hyperparameters set to a batch size of 64, a learning rate of 0.001, a momentum of 0.9, CrossEntropy as loss functions, and SGD as optimization functions. As described above, these models are not fine-tuned to make the side-effect of adversarial training visible.

Figure~\ref{methodology} shows the experimental procedure.

\begin{figure}[H]
\includegraphics[width=13 cm]{Definitions/methodology.png}
\caption{Brief structure of our experiments\label{methodology}}
\end{figure} 

The experiment is composed with two stages.
The first step is data preparation. 
We generated the adversarial dataset by applying FGSM algorithm to the original dataset.
The adversarial examples was applied to both evalutation and training data.
We populated training data by randomly extracting the data from the original training data and the adversarial training data according to the specified ratio while preserving the original total number of training data. The mixing ratio is variated from 10:0 to 6:4(Figure~\ref{methodology} 1-2).

The second step involves training and evaluating the models.
The populated training data generated in the previous step was used for training, wh 
Using populated training data for the model set with the aforementioned hyperparameters, the training proceeded to the epoch in which the accuracy measured during the training process exceeded 90\%.
If the training accuracy does not reach 90\% even after 30 epochs, training is stopped and model is stored without more training. Although models we used have far more capability by being trained for more epochs, we limit its capability to show the effect of adversarial training drastically.
Trained models are evaluated with original(not populated with adversarial examples) validation data and adversarial evaluation data for accuracy and safety, respectively.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussions}

We hypothesized that adversarial training can significantly improve safety while minimizing the impact on performance, and we designed experiments to verify the assumptions.
In this section, we summarize and discuss the results of our experiments.

In our experiment, a dataset without adversarial examples is designated as {\it orig}. The training data and evaluation data in {\it orig} are designated as {\it orig\_train} and {\it orig\_eval}, respectively.
A dataset with adversarial examples, is designated as {\it adv}, and the training data and evaluation data in {\it adv} are designated as {\it adv\_train} and {\it adv\_eval}, respectively.
These designatations refer to each dataset and the number of data contained in each dataset.
The {\it N-model} is a model trained with populated training data containing N\% of adversarial examples in training data.
For example, a model trained without any adversarial training data is called 0-model, and a model trained with populated train data containing 20\% of adversarial training data is called 20-model.
After {\it N-model} is evaluated with evaluation data, the number of successfully classified case is designated by adding {\it correct} in front of each evaluation data.
For example, for evaluation data of {\it orig\_eval}, the number of successfully classified data is termed {\it correct\_orig\_eval}, while for evaluation data of {\it adv\_eval}, the number of successfully classified data is termed {\it correct\_adv\_eval}.

We used accuracy and safety to evaluate the experimental results. Accuracy is a measure of how well the trained model classifies {\it orig\_eval} and can be expressed as follows:

\begin{equation}
    acc(\%) = \text{{\it correct\_orig\_eval}} / \text{{\it orig\_eval}} \times 100
\end{equation} 

Safety is a measure of how well the trained model classifies {\it adv\_eval}, and it can be expressed as follows:

\begin{equation}
    safety(\%) = \text{{\it correct\_adv\_eval}} / \text{{\it adv\_eval}} \times 100
\end{equation}

Although the same equation is applied, we distinguish between the two parameters depending on whether the evaluation data are in {\it orig\_eval} or {\it adv\_eval}.

\subsection{Results from Experiments with CIFAR-10}

\begin{specialtable}[H]
    \centering
    \caption{Results from experiments of CIFAR-10 with \begin{math}\epsilon\end{math}}
    \label{cifar10-result}
    {\small
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Population ratio} & \multirow{2}{*}{Matrix} & \multicolumn{3}{c|}{\begin{math}\epsilon=\end{math}0.05}            & \multicolumn{3}{c|}{\begin{math}\epsilon=\end{math} 0.1}         \\ \cline{3-8} 
                                 &                           & LeNet               & ResNet18              & VGG16              & LeNet              & ResNet18              & VGG16              \\ \hline
    \multirow{2}{*}{10:0}        & Acc                       & 63                  & 53                    & 78                 & 63                 & 53                    & 78                 \\ \cline{2-8} 
                                 & Safety                    & 18                  & 33                    & 33                 & 11                 & 27                    & 27                 \\ \hline
    \multirow{2}{*}{8:2}         & Acc                       & 60                  & 51                    & 73                 & 60                 & 50                    & 74                 \\ \cline{2-8} 
                                 & Safety                    & 45                  & 35                    & 56                 & 38                 & 31                    & 53                 \\ \hline
    \multirow{2}{*}{7:3}         & Acc                       & 58                  & 49                    & 72                 & 57                 & 49                    & 69                 \\ \cline{2-8} 
                                 & Safety                    & 45                  & 36                    & 56                 & 41                 & 34                    & 54                 \\ \hline
    \multirow{2}{*}{6:4}         & Acc                       & 58                  & 48                    & 70                 & 58                 & 47                    & 70                 \\ \cline{2-8} 
                                 & Safety                    & 47                  & 36                    & 56                 & 45                 & 34                    & 56                 \\ \hline
    \end{tabular}
    }
\end{specialtable}

Table~\ref{cifar10-result} lists the CIFAR-10 results according to \begin{math}\epsilon\end{math} for each model.

The results for the evaluation using adversarial examples produced with \begin{math}\epsilon\end{math} of 0.05 show significant difference between accuracy and safety, in case of 0-model the difference is 45\%  for LeNet, 20\% for ResNet18, and 45\% for VGG16. These results mean 0-model has a severe weakness  against adversarial attack.
The 0-model and 20-model showed a minor decrease in accuracy by 3\%(LeNet), 2\%(ResNet18), and 5\%(VGG16) for each model, while significantly increase safety by 27\%(LeNet), 2\%(ResNet18), and 23\%(VGG16).
Although there is a difference between the 30-model and 40-model, the accuracy slightly decreased compared with that of 0-model, and the safety was significantly improved.
Comparing the 20-model, 30-model, and 40-model, lower accuracy and higher safety were observed in the model with higher proportion of adversarial examples, but the differences were not that significant.

In the results for the adversarial examples produced with \begin{math}\epsilon\end{math} of 0.1, as in \begin{math}\epsilon\end{math} of 0.05, the accuracy of the 20-, 30-, 40-model was slightly reduced when compared with the 0-model, but the safety was significantly improved.

\begin{figure}[H]
    \includegraphics[width=13 cm]{Definitions/graph-005cifar10.png}
    \caption{Graphs for experiment with CIFAR-10 using \begin{math}\epsilon\end{math} of 0.05\label{cifar10-0.05-graph}}
\end{figure}

These results are shown in Figure~\ref{cifar10-0.05-graph} and Figure~\ref{cifar10-0.1-graph}.
Figure~\ref{cifar10-0.05-graph} confirms the results for {\it adv\_eval} with \begin{math}\epsilon\end{math} of 0.05.
As shown in Table~\ref{cifar10-result}, when adversarial examples were populated with a higher rate the accuracy was slightly decreased while the safety was significantly improved.

\begin{figure}[H]
    \includegraphics[width=13 cm]{Definitions/graph-01cifar10.png}
    \caption{Graphs for experiment with CIFAR-10 using \begin{math}\epsilon\end{math} of 0.1\label{cifar10-0.1-graph}}
\end{figure}

Figure~\ref{cifar10-0.1-graph} shows the results for {\it adv\_eval} made with \begin{math}\epsilon\end{math} of 0.1.
Similar to Figure~\ref{cifar10-0.05-graph}, the accuracy and safety change as the ratio of populated adversarial examples increases.

\subsection{Results from Experiments with CIFAR-100}

\begin{specialtable}[H]
    \centering
    \caption{Results from experiments of CIFAR-100 with \begin{math}\epsilon\end{math}}
    \label{cifar100-results}
    {\small
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Population ratio} & \multirow{2}{*}{Matrix} & \multicolumn{3}{c|}{\begin{math}\epsilon=\end{math} 0.05}  & \multicolumn{3}{c|}{\begin{math}\epsilon=\end{math} 0.1}       \\ \cline{3-8}
                                 &                           & LeNet               & ResNet18              & VGG16       & LeNet              & ResNet18              & VGG16              \\ \hline 
    \multirow{2}{*}{10:0}        & Acc                       & 27                  & 19                    & 40          & 27                 & 19                    & 40                 \\ \cline{2-8} 
                                 & Safety                    & 15                  & 7                     & 10          & 4                  & 6                     & 7                  \\ \hline 
    \multirow{2}{*}{8:2}         & Acc                       & 26                  & 17                    & 33          & 25                 & 17                    & 35                 \\ \cline{2-8} 
                                 & Safety                    & 15                  & 9                     & 20          & 12                 & 8                     & 21                 \\ \hline 
    \multirow{2}{*}{7:3}         & Acc                       & 26                  & 17                    & 33          & 21                 & 16                    & 29                 \\ \cline{2-8} 
                                 & Safety                    & 15                  & 10                    & 22          & 12                 & 9                     & 20                 \\ \hline 
    \multirow{2}{*}{6:4}         & Acc                       & 24                  & 15                    & 30          & 22                 & 16                    & 26                 \\ \cline{2-8} 
                                 & Safety                    & 18                  & 10                    & 22          & 15                 & 10                    & 20                 \\ \hline 
    \end{tabular}
    }
\end{specialtable}

Table~\ref{cifar100-results} lists the CIFAR-100 results according to \begin{math}\epsilon\end{math} for each model.

As shown above, accuracy and safety for CIFAR-100 is much lower than that those from CIFAR-10, because CIFAR-100 has more classes than CIFAR-10 but less data corresponding to the classes. Since the number of data for training was small, the models were not sufficiently trained within 30 epochs-limit, unlike in other datasets. 
Nevertheless, as in the experimental results of CIFAR-10, as the populate ratio of adversarial examples in the training data increased, the accuracy decreased slightly while the safety improved significantly.
This is confirmed by the results of 0-model and 20-model, where the range of performance change in accuracy and safety can be observed the most.
The decrease in accuracy and improvement in safety were the same in ResNet18, which shows the lowest accuracy and safety due to insufficient training.
Also, the improvement in safety was larger than the decrease in accuracy in LeNet and VGG16, which were well trained when compared with ResNet18.
Similar to the CIFAR-10 results, even in a situation where training is insufficient, safety could be improved with a small impact on the accuracy through adversarial training.

\begin{figure}[H]
    \includegraphics[width=13 cm]{Definitions/graph-005cifar100.png}
    \caption{Graphs of CIFAR-100 with \begin{math}\epsilon\end{math} of 0.05\label{cifar100-0.05-graph}}
\end{figure} 

These result are depicted in Figure~\ref{cifar100-0.05-graph} and \ref{cifar100-0.1-graph}.
Figure~\ref{cifar100-0.05-graph} shows the results for the CIFAR-100 {\it adv\_eval} with \begin{math}\epsilon\end{math} of 0.05.
In some models, the improvement in safety was smaller than the decrease in accuracy because the models were not properly trained.
However, even without proper training, safety increased by 3\%, and accuracy decreases by 2\% for the 30- and 40-model of LeNet,
whereas safety increased by 1\% and accuracy remained unchanged in the 20- and 30-model of ResNet18.

\begin{figure}[H]
    \includegraphics[width=13 cm]{Definitions/graph-01cifar100.png}
    \caption{Graphs of CIFAR-100 with \begin{math}\epsilon\end{math} of 0.1\label{cifar100-0.1-graph}}
\end{figure} 

Figure~\ref{cifar100-0.1-graph} depicts the CIFAR-100 {\it adv\_eval} results with \begin{math}\epsilon\end{math} of 0.1.
In ResNet18 which was the most insufficiently trained, the decrease in accuracy and improvement in safety is the same.
in LeNet and VGG16, the improvement of safety was generally greater than the decrease in accuracy.
CIFAR-100 results show that adversarial training can significantly improve safety with little impact on accuracy in most, if not all cases, even for models that are not properly trained.

\subsection{GTSRB Results}

\begin{specialtable}[H]
    \centering
    \caption{Results of GTSRB with \begin{math}\epsilon\end{math}}
    \label{gtsrb-results}
    {\small
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Population ratio} & \multirow{2}{*}{Matrix} & \multicolumn{3}{c|}{\begin{math}\epsilon=0.05\end{math}}  & \multicolumn{3}{c|}{\begin{math}\epsilon=0.1\end{math}}       \\ \cline{3-8}
                                 &                           & LeNet               & ResNet18              & VGG16       & LeNet              & ResNet18              & VGG16              \\ \hline 
    \multirow{2}{*}{10:0}        & Acc                       & 85                  & 71                    & 93          & 85                 & 71                    & 93                 \\ \cline{2-8} 
                                 & Safety                    & 17                  & 33                    & 40          & 17                 & 33                    & 40                 \\ \hline 
    \multirow{2}{*}{8:2}         & Acc                       & 76                  & 63                    & 90          & 73                 & 60                    & 85                 \\ \cline{2-8} 
                                 & Safety                    & 57                  & 49                    & 75          & 58                 & 42                    & 70                 \\ \hline 
    \multirow{2}{*}{7:3}         & Acc                       & 75                  & 60                    & 86          & 70                 & 58                    & 81                 \\ \cline{2-8} 
                                 & Safety                    & 63                  & 50                    & 76          & 65                 & 45                    & 72                 \\ \hline 
    \multirow{2}{*}{6:4}         & Acc                       & 75                  & 58                    & 87          & 65                 & 54                    & 77                 \\ \cline{2-8} 
                                 & Safety                    & 66                  & 52                    & 79          & 65                 & 46                    & 73                 \\ \hline 
    \end{tabular}
    }
\end{specialtable}

Table~\ref{gtsrb-results} shows the GTSRB results with \begin{math}\epsilon\end{math} for each model.
The result of the 0-model shows that GTSRB exhibited the highest accuracy than the 0-model of the previous dataset but with lower safety. 
The accuracy and safety significantly differ by 68\%(LeNet), 38\%(ResNet18), and 53\%(VGG16).
This difference shows GTSRB has significant weakness against adversarial attack.

Comparing the GTSRB 0- and 20-model, we found that GTSRB gains most by adversarial training. 
Calculating the average of differences in accuracy and safety between 0- and 20-model for each network,
in the case of CIFAR-10 the accuracy of LeNet and ResNet18 decreased by an average of 3\% and 2.5\%, and the safety increased by an average of 27\% and 35\%, respectively. The accuracy of VGG16 decreased by an average of 4.5\%, and the safety increased by an average of 24.5\%.
In the case of CIFAR-100, the accuracy of LeNet, ResNet18, and VGG16 decreased by an average of 1.5\%, 2\%, and 6\%, and their safety increased by an average of 4\%, 2\%, and 12\%, respectively.
In the case of GTSRB, the accuracy of the three models decreased by an average of 10.5\%, 9.5\%, and 5.5\%, and their safety increased by an average of 39.5\%, 12.5\%, and 32.5\%, respectively.


\begin{figure}[H]
    \includegraphics[width=13 cm]{Definitions/graph-005gtsrb.png}
    \caption{Graphs of GTSRB with \begin{math}\epsilon\end{math} of 0.05\label{gtsrb-0.05-graph}}
\end{figure} 

These results are depicted in Figure~\ref{gtsrb-0.05-graph} and \ref{gtsrb-0.1-graph}.
Figure~\ref{gtsrb-0.05-graph} shows the GTSRB {\it adv\_eval} results obtained with \begin{math}\epsilon\end{math} of 0.05.
In most models, except ResNet18, adversarial training could significantly improve safety with a slight decrease in accuracy.

\begin{figure}[H]
    \includegraphics[width=13 cm]{Definitions/graph-01gtsrb.png}
    \caption{Graphs of GTSRB with \begin{math}\epsilon\end{math} of 0.1\label{gtsrb-0.1-graph}}
\end{figure} 

Figure~\ref{gtsrb-0.1-graph} shows the GTSRB {\it adv\_eval} results with \begin{math}\epsilon\end{math} of 0.1.
In both Figure~\ref{gtsrb-0.05-graph} and \ref{gtsrb-0.1-graph} as with the CIFAR datasets, the improvement in safety is much larger than the decrease in accuracy, and there is a slight difference among models with adversarial examples.

\subsection{Discussion}

The experimental results are discussed according to three different datasets and models and various experimental conditions, and the answer to the RQs are provided as follows.
For all datasets, the safety of 0-model largely different with the accuracy. This confirms that the adversarial examples were properly generated, and models are vulnerable to adversarial attacks.

RQ1 is a question of whether adversarial training affects evaluation of adversarial examples.
In experiments using CIFAR datasets, the safety of the 20-, 30-, and 40-model, including the adversarial examples, was significantly improved when compared with that of the model without adversarial examples.
This confirms that adversarial training can improve the safety performance of the classification model.
However, without sufficient training, the improvement in results from CIFAR-100 is limited but the tendency could still be observed in the similar way.

RQ2 is an extension of RQ1. It asks whether the dataset used in the experiment can be benefitted from adversarial training even if it is a dataset related to safety.
Although it varies depending on the size and structure of data, the safety of GTSRB more than the CIFAR datasets.
At the same time, as in CIFAR datasets, the degree of improvement in safety is greater than the degree of decrease in accuracy when adversarial training was performed.
This confirms that adversarial training can improve safety with a small effect on accuracy, even in safety-related datasets. The result shows benefit from adversarial training can be vary depending the characteristics of the data. 

RQ3 asks whether adversarial training affects safety regardless of the structure of the model.
To prove this, models with three different structures were applied in the experiment.
The results from adversarial trainings show that safety was improved for all model, although the benefit might differ.
Although some models used in the experiment may have a greater loss in accuracy when compared with the safety improvements, the overall experimental results show that they can improve safety performance without largely impacting accuracy regardless of the model structure.

RQ4 is a question of what ratio would be the best performance to mix adversarial examples with original data to form a populated training dataset.
This depends on the dataset used and the priority between accuracy and safety. 
Experiments showed that it is essential to mix adversarial examples from the training data, at least 20\% when constructing the training dataset to verify the safety.
This is because the greatest improvement in safety could be obtained compared to the 0-model when the 20-model was used in all experimental environments.
Even when adversarial training data were included at a higher percentage, the degree of safety improvement was small. Thus, it can be considered as the minimum requirement to configure 20\% of the total training dataset as adversarial examples. 
If accuracy has higher priority, the proportion of adversarial examples can be reduced and vice versa. Generally speaking, "best proportion" of adversarial examples might be differ depending characteristic of data, requirement priority, number of data, and so on. Therefore, there should be careful assessment and verification for AI models before deploying them.

Additionally, the shape of the graphs shows the same tendency for all datasets and \begin{math}\epsilon\end{math} values.
This shows that regardless of the proportion of the adversarial example, adversarial training can improve safety.

In summary, adversarial training can improve the safety of classification models regardless of the structure of the model, and is appliable to safety-related datasets.
To verify safety, it is recommended that at least 20\% of the training data comprise adversarial examples, and safety can be verified by adjusting the populate ratio of adversarial examples according to priority between accuracy and safety.
On the basis of these results, we propose a new classification model training process reflecting factors that can verify safety.

\begin{figure}[H]
    \includegraphics[width=13 cm]{Definitions/comparison.png}
    \caption{Comparison between existing processes and our process\label{comparison}}
\end{figure} 

In existing processes, the performance, one of the major AI NFRs, has been verified by training the model using the original dataset and evaluating the trained model.
If the performance does not reach the standard, the same process is repeated until the target performance is achieved through the processes such as modifying the structure of the model, tuning hyperparameters or modifying the dataset.
Due to black-box property of deep neural networks, it is hard to guarantee safety will remain intact  during such process.

We propose a process regarding performance and safety enhancement here.
First of all, an adversarial dataset is created using the original dataset. The \begin{math}\epsilon\end{math} must be decided under human supervision.
Then using the generated adversarial dataset, populated training dataset is constructed with suggested population ratio of 20\% and the model is trained with the dataset.
The trained model is evaluated for accuracy and safety through the original and adversarial evaluation datasets, respectively. If safety is not achieved, add more adversarial examples from pre-generated dataset and repeat the process.
With the proposed method, safety can be used to obtain additional information on the coordination of data by comparing priorities between accuracy and safety.
If accuracy is more important , the populate ratio of the adversarial examples in the training dataset can be decreased, and vice versa.
This does not provide information regarding the model structure or hyperparameters finetuning but provides additional information regarding what augmentation should be applied to the minimal dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

Through experiments, we found that adversarial training can improve the safety performance of models with little impact on accuracy.
This means that generating adversarial examples using the dataset obtained before proceeding with learning and including the adversarial examples in the training dataset can be applied to verify the safety of AI.
Furthermore, we propose a method to achieve a balance between accuracy and safety for the datasets used in experiments by generating a populated dataset of different ratios.
Using the proposed method, a balance between accuracy and safety can be achieved by performing the experiment several times according to the important requirements and adjusting the populate ratio of adversarial examples in the training dataset.
Thus the required resources can be reduced at least for the training process.
Moreover, the cost of training models include verifying other NFRs can be reduced by adjusting the ratio of inclusion of adversarial examples for higher-value requirements through accuracy and safety after one training and evaluation.

In future studies, we shall verify the validity of the proposed process for more NFRs, like privacy or fairness. Different model structures and various adversarial attack methods can also be applied to validate our process.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following are available online at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title.}

% Only for the journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following are available at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{For research articles with several authors, a short paragraph specifying their individual contributions must be provided. The following statements should be used ``Conceptualization, X.X. and Y.Y.; methodology, X.X.; software, X.X.; validation, X.X., Y.Y. and Z.Z.; formal analysis, X.X.; investigation, X.X.; resources, X.X.; data curation, X.X.; writing---original draft preparation, X.X.; writing---review and editing, X.X.; visualization, X.X.; supervision, X.X.; project administration, X.X.; funding acquisition, Y.Y. All authors have read and agreed to the published version of the manuscript.'', please turn to the  \href{http://img.mdpi.org/data/contributor-role-instruction.pdf}{CRediT taxonomy} for the term explanation. Authorship must be limited to those who have contributed substantially to the work~reported.}

\funding{``This research received no external funding''}

% \institutionalreview{In this section, please add the Institutional Review Board Statement and approval number for studies involving humans or animals. Please note that the Editorial Office might ask you for further information. Please add ``The study was conducted according to the guidelines of the Declaration of Helsinki, and approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).'' OR ``Ethical review and approval were waived for this study, due to REASON (please provide a detailed justification).'' OR ``Not applicable'' for studies not involving humans or animals. You might also choose to exclude this statement if the study did not involve humans or animals.}

% \informedconsent{Any research article describing a study involving humans should contain this statement. Please add ``Informed consent was obtained from all subjects involved in the study.'' OR ``Patient consent was waived due to REASON (please provide a detailed justification).'' OR ``Not applicable'' for studies not involving humans. You might also choose to exclude this statement if the study did not involve humans.

% Written informed consent for publication must be obtained from participating patients who can be identified (including by the patients themselves). Please state ``Written informed consent has been obtained from the patient(s) to publish this paper'' if applicable.}

% \dataavailability{In this section, please provide details regarding where data supporting reported results can be found, including links to publicly archived datasets analyzed or generated during the study. Please refer to suggested Data Availability Statements in section ``MDPI Research Data Policies'' at \url{https://www.mdpi.com/ethics}. You might choose to exclude this statement if the study did not report any data.} 

% \acknowledgments{In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments).}

% \conflictsofinterest{Declare conflicts of interest or state ``The authors declare no conflict of interest.'' Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results. Any role of the funders in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript, or in the decision to publish the results must be declared in this section. If there is no role, please state ``The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the~results''.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
% \appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
% \appendixstart
% \appendix
% \section{}
% \subsection{}
% The appendix is an optional section that can contain details and data supplemental to the main text---for example, explanations of experimental details that would disrupt the flow of the main text but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data are shown in the main text can be added here if brief, or as Supplementary Data. Mathematical proofs of results not central to the paper can be added as an appendix.

% \section{}
% All appendix sections must be cited in the main text. In the appendices, Figures, Tables, etc. should be labeled, starting with ``A''---e.g., Figure A1, Figure A2, etc. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{paracol}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To add notes in main text, please use \endnote{} and un-comment the codes below.
%\begin{adjustwidth}{-5.0cm}{0cm}
%\printendnotes[custom]
%\end{adjustwidth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
%\externalbibliography{yes}
%\bibliography{your_external_BibTeX_file}

%=====================================
% References, variant B: internal bibliography
\begin{thebibliography}{999}
% NFRS - fairness
\bibitem[Bird, S(2020)]{fairness-microsoft}
Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., Sameki, M., Wallach, H., Walker, K., Fairlearn: A toolkit for assessing and improving fairness in AI. {\em Microsoft, Tech. Rep.} {\bf 2020}, {\em MSR-TR-2020-32}; 142--149.
\bibitem[Dwork, C(2012)]{fairness-dwork}
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R., Fairness through awareness. Proceedings of the 3rd innovations in theoretical computer science conference; pp. 214--226.
\bibitem[Feldman, M(2015)]{fairness-feldman}
Feldmen, M., Friedler, S. A., Moeller, J., Scheidegger, C., Venkatasubramanian, S., Certifying and removing disparate impact. Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining; pp 259--268.
\bibitem[Tramer, F(2017)]{fairness-tramer}
Tramer, F., Atildakis, V., Geambasu, R., Hsu, D., Hubaux, J. P., Humbert, M, ..., Lin, H., Fairtest: Discovering unwarranted associations in data-driven applications. In IEEE European Symposium on SEcurity and Privacy, IEEE.
\bibitem[Zhang, J(2021)]{fairness-zhang} 
Zhang, J., Harman, M., Ignorance and Prejudice. In 2021 IEEE/ACM 43rd International Conference on Software Engineering(ICSE), IEEE.
\bibitem[Zemel, R.(2013)]{fairness-zemel}
Zemel, R., Wu, Y., Swersky, K., Pitassi, T., Dwork. C., Learning fair representations. International conference on machine learning; pp. 325--333.
% NFRS - transparency & xai
\bibitem[Yosinski, J(2015)]{transparency-yosinski}
Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., Lipson, H., Understanding neural networks through deep visualization. \textit{arXiv preprint arXiv:1506.06579.}; 2015.
\bibitem[Samek, W(2017)]{xai-samek}
Samek, W., Wiegand, T., Müller, K. R., Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. \textit{arXiv preprint arXiv:1708.08296.} 2017.
\bibitem[Arrieta, A. B.(2020)]{xai-arrieta}
Arrieta, A. B., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garcia, S., Gil-Lopez, S., Molina, D., Benjamins, R., Chatila, R., Herrera, F., Explainable Artificial Intelligence(XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. {\em Information Fusion} {\bf 2020}, {\em 58}, 82--115.
\bibitem[Ribeiro, M. T.(2016)]{xai-transparency-ribeiro}
Ribeiro, M. T., Singh, S., Guestrin, C., "Why should i trust you?" Explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining; 1135--1144.
\bibitem[Merdoch, W. J.(2019)]{xai-transparency-murdoch}
Murdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asi, R., Yu, B., Interpretable machine learning: definitions, methods, and applications. \textit{arXiv preprint arXiv:1901.04592.}; 2019.
% NFRS - security
\bibitem[Mei, S(2015)]{security-mei}
Mei, S., Zhu, X., The security of latent dirichlet allocation. Artificial Intelligence and Statistics; PMLR, 2015; pp 681--689.
\bibitem[Mei, S(2015)]{security-mei2}
Mei, S., Zhu, X., Using machine teaching to identify optimal training-set attakcs on machine learners. Twenty-Ninth AAAI Conference on Artificial Intelligence.
\bibitem[Barreno, M(2010)]{security-barreno}
Barreno, M., Nelson, B., Joseph, A. D., Tygar, J. D.,  The security of machine learning. {\em Machine Learning, 81} {\bf 2010}, {\em 2}, 121--148.
% NFRS - safety
\bibitem[Amodei, D(2016)]{safety-amodei}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., Mané, D., Concrete problems in AI safety. \textit{arXiv preprint arXiv:1606.06565.}; 2016.
\bibitem[Juric, M.(2020)]{safety-juric}
Juric, M., Sandic, A., Brcic, M., AI safety: state of the field through quantitative lens. 2020 43rd International Convention on Information, Communication and Electronic Technology(MIPRO); 1254--1259.
\bibitem[Leike, J.(2017)]{safety-leike}
Leike, J., Martic, M., Krakovna, V., Ortega, P. A., Everitt, T., Lefrancq, A., ..., Legg, S., AI safety gridworlds. \textit{arXiv preprint arXiv:1711.09883.}; 2017.
% AI APPLICATIONS
\bibitem[Bojarski, M(2016)]{ai-driving-bajarski}
Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., ..., Zieba, K., End to end learning to self-driving cars. \textit{arXiv preprint arXiv:1604.07316.}; 2016.
\bibitem[Levinson, J(2011)]{ai-driving-levinson}
Levinson, J., Askeland, J., Becker, J., Dolson, J., Held, D., Kammel, S., ..., Thrun, S., Towards fully autonomous driving: Systems and algorithms. 2011 IEEE intelligent vehicles symposium; 163--168.
\bibitem[Fu, K(2016)]{ai-finance-fu}
Fu, K., Cheng, D., Tu, Y., Zhang, L., Credit card fraud detection using convolutional neural networks. International conference on neural information processing; Springer, Cham, 2016; pp 483-490.
\bibitem[Vieira, S(2017)]{ai-medical-vieira}
Vieira, S., Pinaya, W. H., Mechelli, A., Using deep learning to investigate the neuroimaging correlates of psychiatric and neurologicla disorders: Methods and applications. {\em Neuroscience \& Biobehavioral Reviews} {\bf 2017}, {\em 74}, 58--75.
\bibitem[Ramsundar, B(2015)]{ai-medical-ramsundar}
Ramsundar, B., Kearnes, S., Riley, P., Webster, D., Konerding, D., Pande, V., Massively multitask networks for drug discovery. \textit{arXiv preprint arXiv:1502.02072.}; 2015.
\bibitem[Holzinger, A(2017)]{ai-medical-xai-holzinger}
Holzinger, A., Biemann, C., Pattichis, C. S., Kell, D. B., What do we need to build explainable AI systems for the medical domain?. \textit{arXiv preprint arXiv:1712.09923.}; 2017.
\bibitem[Krause, J.(2016)]{ai-medical-krause}
Krause, J., Pere, A., Ng, K., Interacting with predictions: Visual inspection of black-box machine learning models. Proceedings of the 2016 CHI conference on human factors in computing systems; 5686--5697.
\bibitem[Tan, J.(2014)]{ai-medical-tan}
Tan, J., Ung, M., Cheng, C., Greence, C. S., Unsupervised feature contruction and knowledge extraction from genome-wide assays of breast cancer with denoising autoencoders. Pacific symposium on biocomputing co-charis; 132--143.
\bibitem[Pesapane, F.(2018)]{ai-medical-pesapane}
Pesapane, F., Volonté, C., Codari, M., Sardanelli, F., Artificial intelligence as a medical device in radiology: ethical and regulatory: ethical and regulatory issues in Europe and the United States. {\em Insights into imaging} {\bf 2018}, {\em 9(5)}, 743--753.
\bibitem[Miller, D. D.(2018)]{ai-medical-miller}
Miller, D. D., Brown, E. W., Artificial intelligence in medical practice: the question to the answer?. {\em The American journal of medicine} {\bf 2018}, {\em 131(2)}, 129--133.
% ADVERSARIAL TRAINING
\bibitem[Kurakin, A(2018)]{adversarial-kurakin}
Kurakin, A., Goodfellow, I., Bengio, S., Dong, Y., Liao, F., Liang, M., ..., Abe, M., Adversarial attacks and defences competition. The NIPS'17 Competition: Building Intelligent Systems; Springer, Cham, 2018; pp 195--231.
\bibitem[Szegedy, C(2013)]{adversarial-szegedy}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R., Intriguing properties of neural networks. \textit{arXiv preprint arXiv:1312.6199.}; 2013.
\bibitem[Goodfellow, I(2014)]{adversarial-goodfellow}
Goodfellow, I., Shlens, J., Szegedy, C., Explaining and harnessing adversarial examples. \textit{arXiv preprint arXiv:1412.6572.}; 2014.
\bibitem[Kurakin, A(2016)]{adversarial-kurakin2}
Kurakin, A., Goodfellow, I., Bengio, S., Adversarial examples in the physical world. \textit{arXiv preprint arXiv:1607.02533.}; 2016.
\bibitem[Papernot, N(2017)]{adversarial-papernot}
Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., Swami, A., Practical black-box attacks against machine learning. Proceedings of the 2017 ACM on Asia conference on computer and communications security; pp 506--519.
\bibitem[Yuan, X.(2019)]{adversarial-yuan}
Yuan, X., He, P., Zhu, Q., Li, X., Adversarial examples: Attacks and defenses for deep learning. {\em IEEE transactions on neural networks and learning systems} {\bf 2019}, {\em 30(9)}, 2805--2824.
\bibitem[Poon, H(2019)]{adversarial-poon}
Poon, H.-K.,  Yap, W.-S., Tee, Y.-K.,   Lee, W.-K., Goi, B.-M., Hierarchical gated recurrent neural network with adversarial and virtual adversarial training on text classification, {\em Neural Networks} {\bf 2019}, {\em 119},  299--312.
\bibitem[Terzi, M(2020)]{adversarial-terzi}
Terzi, M., Susto, G. A., Chadhari, P., Directional adversarial training for cost sensitive deep learning classification applications, {\em Engineering Applications of Artificial Intelligence} {\bf 2020}, {\em 91}, 103550.
\bibitem[]{adversarial-dong}
Dong X., Zhu Y., Zhang Y., Fu Z., Xu D., Yang S., Melo G., Leveraging adversarial training in self-learning for cross-lingual text classification, {\em Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval} {\bf 2020}, 1541--1544.

% ETC
\bibitem[Paszke, A(2019)]{pytorch}
Paszke, A., Gross, S., Massa. F., Lerer, A., Bradbury, J., Channan, G., ..., Chintala, S., Pytorch: An imperative style, high performance deep leawrning library. Advances in neural information processing systems, 32; pp 8026--8037.
\bibitem[Krizhevsky, A(2009)]{cifar}
Krizhevskey, A., Hinton, G., Leawrning multiple layers of features from tiny images. 2009.
\bibitem[Stallkamp, J(2012)]{gtsrb}
Stallkamp, J., Schlipsing, M., Salmen, J., Igel, C., Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. {\em Neural networks} {\bf 2012}, {\em 32}, 323--332.
\bibitem[LeCun, Y(1995)]{lenet}
LeCun, Y., Bottou, L., Haffner, P., Gradient-based learning applied to document recognition. Proceedings of the IEEE; 86(11); 2278--2324.
\bibitem[He, K(2016)]{resnet}
He, K., Zhang, X., Ren, S., Sun, J., Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition; 770--778.
\bibitem[Simonyan, K(2014)]{vgg}
Simonyan, K., Zisserman, A., Very deep convolutional networks for large-scale image recognition. \textit{arXiv preprint arXiv:1409.1556.}; 2014.
\bibitem[Ajunwa, I(2016)]{trade-off}
Ajunwa, I., Friedler, S., Scheidegeer, C. E., Venkatasubramanian, S., Hiring by algorithm: predicting and preventing disparate impack. \textit{Available at SSRN.}; 2016.
%%%% template
% Reference 1
% \bibitem[Author1(year)]{ref-journal}
% Author~1, T. The title of the cited article. {\em Journal Abbreviation} {\bf 2008}, {\em 10}, 142--149.
% Reference 2
% \bibitem[Author2(year)]{ref-book1}
% Author~2, L. The title of the cited contribution. In {\em The Book Title}; Editor1, F., Editor2, A., Eds.; Publishing House: City, Country, 2007; pp. 32--58.
% \bibitem[Author3(year)]{ref-book2}
% Author 1, A.; Author 2, B. \textit{Book Title}, 3rd ed.; Publisher: Publisher Location, Country, 2008; pp. 154--196.
% Reference 4
% \bibitem[Author4(year)]{ref-unpublish}
% Author 1, A.B.; Author 2, C. Title of Unpublished Work. \textit{Abbreviated Journal Name} stage of publication (under review; accepted; in~press).
% Reference 5
% \bibitem[Author5(year)]{ref-communication}
% Author 1, A.B. (University, City, State, Country); Author 2, C. (Institute, City, State, Country). Personal communication, 2012.
% Reference 6
% \bibitem[Author6(year)]{ref-proceeding}
% Author 1, A.B.; Author 2, C.D.; Author 3, E.F. Title of Presentation. In Title of the Collected Work (if available), Proceedings of the Name of the Conference, Location of Conference, Country, Date of Conference; Editor 1, Editor 2, Eds. (if available); Publisher: City, Country, Year (if available); Abstract Number (optional), Pagination (optional).
% Reference 7
% \bibitem[Author7(year)]{ref-thesis}
% Author 1, A.B. Title of Thesis. Level of Thesis, Degree-Granting University, Location of University, Date of Completion.
% Reference 8
% \bibitem[Author8(year)]{ref-url}
% Title of Site. Available online: URL (accessed on Day Month Year).
\end{thebibliography}

% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% The following MDPI journals use author-date citation: Admsci,  Arts, Econometrics, Economies, Genealogy, Humanities, IJFS, Jintelligence, JRFM, Languages, Laws, Literature, Religions, Risks, Social Sciences. For those journals, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

